"""
üöÄ SCRAPER BDE HELLOASSO - VERSION FORC√âE MULTI-PAGES
Auteur: Assistant IA
Description: Script pour scraper les BDE sur HelloAsso en for√ßant l'acc√®s √† toutes les pages
"""

import os
import time
import csv
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from tqdm import tqdm

# Configuration
BASE_URL = "https://www.helloasso.com/associations"
SEARCH_PARAMS = {
    'category_tags': 'bde'
}
OUTPUT_FILENAME = "bde_scraping_results_forced"
DELAY_BETWEEN_REQUESTS = 2
MAX_PAGES_TO_TRY = 20  # On va essayer jusqu'√† 20 pages

class BDEScraperForced:
    def __init__(self):
        self.driver = None
        self.bde_data = []
        self.current_page = 1
        
        # Cr√©ation du dossier data
        os.makedirs('data', exist_ok=True)
    
    def setup_driver(self):
        """
        Configure le navigateur Chrome en mode headless
        """
        print("üîß Configuration du navigateur...")
        
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            print("‚úÖ Navigateur configur√© avec succ√®s")
        except Exception as e:
            print(f"‚ùå Erreur lors de la configuration : {str(e)}")
            raise

    def get_page_url(self, page_number):
        """
        G√©n√®re l'URL pour une page sp√©cifique
        """
        if page_number == 1:
            return f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}"
        else:
            # Diff√©rentes variations d'URL possible pour la pagination
            possible_urls = [
                f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}&page={page_number}",
                f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}&p={page_number}",
                f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}&offset={20*(page_number-1)}",
                f"{BASE_URL}/search?query=bde&page={page_number}",
                f"{BASE_URL}/search?q=bde&page={page_number}",
            ]
            return possible_urls
    
    def get_bde_links_from_page(self):
        """
        Extrait tous les liens des BDE pr√©sents sur la page actuelle
        """
        try:
            print("üîç Recherche des BDE sur la page...")
            
            # Attendre que les √©l√©ments se chargent
            time.sleep(3)
            
            # Diff√©rents s√©lecteurs possibles pour les liens des BDE
            selectors = [
                "a[href*='/associations/']",
                ".association-card a",
                ".card a[href*='associations']",
                "a[href*='bde']",
                ".result-item a"
            ]
            
            all_links = []
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        href = element.get_attribute('href')
                        if href and '/associations/' in href and href not in all_links:
                            # Filtrer pour ne garder que les vrais liens de BDE
                            if any(keyword in href.lower() for keyword in ['bde', 'bureau', 'etudiant', 'eleve']):
                                all_links.append(href)
                except:
                    continue
            
            # Si on n'a pas trouv√© avec les s√©lecteurs sp√©cifiques, on prend tous les liens d'associations
            if not all_links:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/associations/']")
                    for element in elements:
                        href = element.get_attribute('href')
                        if href and href not in all_links:
                            all_links.append(href)
                except:
                    pass
            
            print(f"üîó {len(all_links)} liens trouv√©s sur cette page")
            return list(set(all_links))  # Supprimer les doublons
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'extraction des liens : {str(e)}")
            return []
    
    def extract_bde_details(self, bde_url):
        """
        Extrait les d√©tails d'un BDE sp√©cifique
        """
        try:
            print(f"üìÑ Extraction des d√©tails pour : {bde_url}")
            self.driver.get(bde_url)
            time.sleep(2)
            
            # Structure de base des informations BDE
            bde_info = {
                'nom_ecole': '',
                'nom_personne': '',
                'prenom_personne': '',
                'adresse': '',
                'site_internet': '',
                'telephone': '',
                'email': '',
                'url_source': bde_url
            }
            
            # Extraction du nom de l'√©cole/BDE
            try:
                selectors = ["h1", ".title", ".name", ".association-name", "title"]
                for selector in selectors:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        name = elements[0].text.strip()
                        if name:
                            bde_info['nom_ecole'] = name
                            break
            except:
                pass
            
            # Extraction de l'email
            try:
                page_source = self.driver.page_source
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails = re.findall(email_pattern, page_source)
                if emails:
                    # Filtrer les emails valides et √©viter les emails techniques
                    valid_emails = []
                    for email in emails:
                        if not any(x in email.lower() for x in ['noreply', 'no-reply', 'support', 'admin', 'info@helloasso']):
                            valid_emails.append(email)
                    if valid_emails:
                        bde_info['email'] = valid_emails[0]
            except:
                pass
            
            # Extraction du t√©l√©phone
            try:
                page_text = self.driver.page_source
                phone_patterns = [
                    r'(?:(?:\+33|0)[1-9](?:[0-9]{8}))',
                    r'(?:0[1-9](?:\s?\d{2}){4})',
                    r'(?:\+33\s?[1-9](?:\s?\d{2}){4})'
                ]
                
                for pattern in phone_patterns:
                    phone_matches = re.findall(pattern, page_text)
                    if phone_matches:
                        bde_info['telephone'] = phone_matches[0].strip()
                        break
            except:
                pass
            
            # Extraction du site internet
            try:
                external_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href^='http']")
                for link in external_links:
                    href = link.get_attribute('href')
                    if href and 'helloasso.com' not in href:
                        social_domains = ['facebook.com', 'twitter.com', 'instagram.com', 'linkedin.com', 'youtube.com']
                        if not any(domain in href for domain in social_domains):
                            bde_info['site_internet'] = href
                            break
            except:
                pass
            
            # Extraction de l'adresse
            try:
                page_text = self.driver.page_source
                # Patterns pour adresses fran√ßaises
                address_patterns = [
                    r'\d+[,\s]+(?:rue|avenue|boulevard|place|impasse|all√©e)[^,\n]+(?:\d{5})[^,\n]*',
                    r'(?:rue|avenue|boulevard|place|impasse|all√©e)[^,\n]+(?:\d{5})[^,\n]*'
                ]
                
                for pattern in address_patterns:
                    addresses = re.findall(pattern, page_text, re.IGNORECASE)
                    if addresses:
                        bde_info['adresse'] = addresses[0].strip()
                        break
            except:
                pass
            
            # Affichage des informations extraites
            print(f"   ‚úÖ Donn√©es extraites :")
            for key, value in bde_info.items():
                if value and key != 'url_source':
                    print(f"      {key}: {value}")
            
            return bde_info
            
        except Exception as e:
            print(f"   ‚ùå Erreur lors de l'extraction : {str(e)}")
            return None
    
    def check_page_exists(self, page_url):
        """
        V√©rifie si une page existe et contient du contenu
        """
        try:
            self.driver.get(page_url)
            time.sleep(2)
            
            # V√©rifier si la page contient des associations
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/associations/']")
            return len(elements) > 0
            
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la v√©rification de la page : {str(e)}")
            return False
    
    def save_to_csv(self):
        """
        Sauvegarde les donn√©es en format CSV
        """
        if not self.bde_data:
            print("‚ùå Aucune donn√©e √† sauvegarder")
            return False
            
        try:
            df = pd.DataFrame(self.bde_data)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"data/{OUTPUT_FILENAME}_{timestamp}.csv"
            
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"üíæ Donn√©es sauvegard√©es dans : {filename}")
            print(f"üìä Total : {len(self.bde_data)} BDE scrap√©s")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde : {str(e)}")
            return False
    
    def run_forced_scraping(self, start_page=2, max_pages=MAX_PAGES_TO_TRY):
        """
        Lance le scraping forc√© √† partir d'une page donn√©e
        """
        print("üöÄ D√âBUT DU SCRAPING FORC√â DES BDE")
        print(f"üìÑ Pages {start_page} √† {max_pages}")
        
        try:
            self.setup_driver()
            
            for page_num in range(start_page, max_pages + 1):
                print(f"\nüìÑ === PAGE {page_num} ===")
                
                # Essayer diff√©rentes variations d'URL pour cette page
                page_urls = self.get_page_url(page_num)
                
                # Si c'est la page 1, on a une seule URL
                if isinstance(page_urls, str):
                    page_urls = [page_urls]
                
                page_found = False
                bde_links = []
                
                for url in page_urls:
                    print(f"üîó Tentative : {url}")
                    
                    if self.check_page_exists(url):
                        print("‚úÖ Page trouv√©e !")
                        bde_links = self.get_bde_links_from_page()
                        if bde_links:
                            page_found = True
                            break
                    else:
                        print("‚ùå Page non trouv√©e ou vide")
                
                if not page_found or not bde_links:
                    print(f"üìÑ Page {page_num} : Aucun contenu trouv√©")
                    continue
                
                # Traitement des BDE trouv√©s
                print(f"üîÑ Traitement de {len(bde_links)} BDE...")
                
                for i, bde_url in enumerate(tqdm(bde_links, desc=f"Page {page_num}")):
                    print(f"\n   üìÑ BDE {i+1}/{len(bde_links)}")
                    
                    bde_info = self.extract_bde_details(bde_url)
                    if bde_info:
                        self.bde_data.append(bde_info)
                    
                    time.sleep(DELAY_BETWEEN_REQUESTS)
            
            # Sauvegarde finale
            print(f"\nüíæ SAUVEGARDE DES DONN√âES")
            self.save_to_csv()
            
            print(f"\n‚úÖ SCRAPING FORC√â TERMIN√â !")
            print(f"üìä {len(self.bde_data)} BDE trait√©s au total")
            
        except Exception as e:
            print(f"‚ùå ERREUR CRITIQUE : {str(e)}")
            
        finally:
            if self.driver:
                print("\nüîí Fermeture du navigateur...")
                self.driver.quit()

def main():
    """
    Fonction principale
    """
    print("=" * 60)
    print("üéì SCRAPER BDE HELLOASSO - FOR√áAGE MULTI-PAGES")
    print("=" * 60)
    
    scraper = BDEScraperForced()
    
    # Lancement du scraping forc√© √† partir de la page 2
    scraper.run_forced_scraping(start_page=2, max_pages=10)
    
    print("\n" + "=" * 60)
    print("‚úÖ FIN DU PROGRAMME")
    print("=" * 60)

if __name__ == "__main__":
    main() 