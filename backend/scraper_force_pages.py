"""
ğŸš€ SCRAPER BDE HELLOASSO - VERSION FORCÃ‰E MULTI-PAGES
Auteur: Assistant IA
Description: Script pour scraper les BDE sur HelloAsso en forÃ§ant l'accÃ¨s Ã  toutes les pages
"""

import os
import time
import csv
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from tqdm import tqdm

# Configuration
BASE_URL = "https://www.helloasso.com/associations"
SEARCH_PARAMS = {
    'category_tags': 'bde'
}
OUTPUT_FILENAME = "bde_scraping_results_forced"
DELAY_BETWEEN_REQUESTS = 2
MAX_PAGES_TO_TRY = 20  # On va essayer jusqu'Ã  20 pages

class BDEScraperForced:
    def __init__(self):
        self.driver = None
        self.bde_data = []
        self.current_page = 1
        
        # CrÃ©ation du dossier data
        os.makedirs('data', exist_ok=True)
    
    def setup_driver(self):
        """
        Configure le navigateur Chrome en mode headless
        """
        print("ğŸ”§ Configuration du navigateur...")
        
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.implicitly_wait(10)
            print("âœ… Navigateur configurÃ© avec succÃ¨s")
        except Exception as e:
            print(f"âŒ Erreur lors de la configuration : {str(e)}")
            raise

    def get_page_url(self, page_number):
        """
        GÃ©nÃ¨re l'URL pour une page spÃ©cifique
        """
        if page_number == 1:
            return f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}"
        else:
            # DiffÃ©rentes variations d'URL possible pour la pagination
            possible_urls = [
                f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}&page={page_number}",
                f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}&p={page_number}",
                f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}&offset={20*(page_number-1)}",
                f"{BASE_URL}/search?query=bde&page={page_number}",
                f"{BASE_URL}/search?q=bde&page={page_number}",
            ]
            return possible_urls
    
    def get_bde_links_from_page(self):
        """
        Extrait tous les liens des BDE prÃ©sents sur la page actuelle
        """
        try:
            print("ğŸ” Recherche des BDE sur la page...")
            
            # Attendre que les Ã©lÃ©ments se chargent
            time.sleep(3)
            
            # DiffÃ©rents sÃ©lecteurs possibles pour les liens des BDE
            selectors = [
                "a[href*='/associations/']",
                ".association-card a",
                ".card a[href*='associations']",
                "a[href*='bde']",
                ".result-item a"
            ]
            
            all_links = []
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        href = element.get_attribute('href')
                        if href and '/associations/' in href and href not in all_links:
                            # Filtrer pour ne garder que les vrais liens de BDE
                            if any(keyword in href.lower() for keyword in ['bde', 'bureau', 'etudiant', 'eleve']):
                                all_links.append(href)
                except:
                    continue
            
            # Si on n'a pas trouvÃ© avec les sÃ©lecteurs spÃ©cifiques, on prend tous les liens d'associations
            if not all_links:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/associations/']")
                    for element in elements:
                        href = element.get_attribute('href')
                        if href and href not in all_links:
                            all_links.append(href)
                except:
                    pass
            
            print(f"ğŸ”— {len(all_links)} liens trouvÃ©s sur cette page")
            return list(set(all_links))  # Supprimer les doublons
            
        except Exception as e:
            print(f"âŒ Erreur lors de l'extraction des liens : {str(e)}")
            return []
    
    def extract_bde_details(self, bde_url):
        """
        Extrait les dÃ©tails d'un BDE spÃ©cifique
        """
        try:
            print(f"ğŸ“„ Extraction des dÃ©tails pour : {bde_url}")
            self.driver.get(bde_url)
            time.sleep(2)
            
            # Structure de base des informations BDE
            bde_info = {
                'nom_ecole': '',
                'nom_personne': '',
                'prenom_personne': '',
                'adresse': '',
                'site_internet': '',
                'telephone': '',
                'email': '',
                'url_source': bde_url
            }
            
            # Extraction du nom de l'Ã©cole/BDE
            try:
                selectors = ["h1", ".title", ".name", ".association-name", "title"]
                for selector in selectors:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        name = elements[0].text.strip()
                        if name:
                            bde_info['nom_ecole'] = name
                            break
            except:
                pass
            
            # Extraction de l'email
            try:
                page_source = self.driver.page_source
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails = re.findall(email_pattern, page_source)
                if emails:
                    # Filtrer les emails valides et Ã©viter les emails techniques
                    valid_emails = []
                    for email in emails:
                        if not any(x in email.lower() for x in ['noreply', 'no-reply', 'support', 'admin', 'info@helloasso']):
                            valid_emails.append(email)
                    if valid_emails:
                        bde_info['email'] = valid_emails[0]
            except:
                pass
            
            # Extraction du tÃ©lÃ©phone
            try:
                page_text = self.driver.page_source
                phone_patterns = [
                    r'(?:(?:\+33|0)[1-9](?:[0-9]{8}))',
                    r'(?:0[1-9](?:\s?\d{2}){4})',
                    r'(?:\+33\s?[1-9](?:\s?\d{2}){4})'
                ]
                
                for pattern in phone_patterns:
                    phone_matches = re.findall(pattern, page_text)
                    if phone_matches:
                        bde_info['telephone'] = phone_matches[0].strip()
                        break
            except:
                pass
            
            # Extraction du site internet
            try:
                external_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href^='http']")
                for link in external_links:
                    href = link.get_attribute('href')
                    if href and 'helloasso.com' not in href:
                        social_domains = ['facebook.com', 'twitter.com', 'instagram.com', 'linkedin.com', 'youtube.com']
                        if not any(domain in href for domain in social_domains):
                            bde_info['site_internet'] = href
                            break
            except:
                pass
            
            # Extraction de l'adresse
            try:
                page_text = self.driver.page_source
                # Patterns pour adresses franÃ§aises
                address_patterns = [
                    r'\d+[,\s]+(?:rue|avenue|boulevard|place|impasse|allÃ©e)[^,\n]+(?:\d{5})[^,\n]*',
                    r'(?:rue|avenue|boulevard|place|impasse|allÃ©e)[^,\n]+(?:\d{5})[^,\n]*'
                ]
                
                for pattern in address_patterns:
                    addresses = re.findall(pattern, page_text, re.IGNORECASE)
                    if addresses:
                        bde_info['adresse'] = addresses[0].strip()
                        break
            except:
                pass
            
            # Affichage des informations extraites
            print(f"   âœ… DonnÃ©es extraites :")
            for key, value in bde_info.items():
                if value and key != 'url_source':
                    print(f"      {key}: {value}")
            
            return bde_info
            
        except Exception as e:
            print(f"   âŒ Erreur lors de l'extraction : {str(e)}")
            return None
    
    def check_page_exists(self, page_url):
        """
        VÃ©rifie si une page existe et contient du contenu
        """
        try:
            self.driver.get(page_url)
            time.sleep(2)
            
            # VÃ©rifier si la page contient des associations
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/associations/']")
            return len(elements) > 0
            
        except Exception as e:
            print(f"   âŒ Erreur lors de la vÃ©rification de la page : {str(e)}")
            return False
    
    def save_to_csv(self):
        """
        Sauvegarde les donnÃ©es en format CSV
        """
        if not self.bde_data:
            print("âŒ Aucune donnÃ©e Ã  sauvegarder")
            return False
            
        try:
            df = pd.DataFrame(self.bde_data)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"data/{OUTPUT_FILENAME}_{timestamp}.csv"
            
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"ğŸ’¾ DonnÃ©es sauvegardÃ©es dans : {filename}")
            print(f"ğŸ“Š Total : {len(self.bde_data)} BDE scrapÃ©s")
            
            return True
            
        except Exception as e:
            print(f"âŒ Erreur lors de la sauvegarde : {str(e)}")
            return False
    
    def run_forced_scraping(self, start_page=2, max_pages=MAX_PAGES_TO_TRY):
        """
        Lance le scraping forcÃ© Ã  partir d'une page donnÃ©e
        """
        print("ğŸš€ DÃ‰BUT DU SCRAPING FORCÃ‰ DES BDE")
        print(f"ğŸ“„ Pages {start_page} Ã  {max_pages}")
        
        try:
            self.setup_driver()
            
            for page_num in range(start_page, max_pages + 1):
                print(f"\nğŸ“„ === PAGE {page_num} ===")
                
                # Essayer diffÃ©rentes variations d'URL pour cette page
                page_urls = self.get_page_url(page_num)
                
                # Si c'est la page 1, on a une seule URL
                if isinstance(page_urls, str):
                    page_urls = [page_urls]
                
                page_found = False
                bde_links = []
                
                for url in page_urls:
                    print(f"ğŸ”— Tentative : {url}")
                    
                    if self.check_page_exists(url):
                        print("âœ… Page trouvÃ©e !")
                        bde_links = self.get_bde_links_from_page()
                        if bde_links:
                            page_found = True
                            break
                    else:
                        print("âŒ Page non trouvÃ©e ou vide")
                
                if not page_found or not bde_links:
                    print(f"ğŸ“„ Page {page_num} : Aucun contenu trouvÃ©")
                    continue
                
                # Traitement des BDE trouvÃ©s
                print(f"ğŸ”„ Traitement de {len(bde_links)} BDE...")
                
                for i, bde_url in enumerate(tqdm(bde_links, desc=f"Page {page_num}")):
                    print(f"\n   ğŸ“„ BDE {i+1}/{len(bde_links)}")
                    
                    bde_info = self.extract_bde_details(bde_url)
                    if bde_info:
                        self.bde_data.append(bde_info)
                    
                    time.sleep(DELAY_BETWEEN_REQUESTS)
            
            # Sauvegarde finale
            print(f"\nğŸ’¾ SAUVEGARDE DES DONNÃ‰ES")
            self.save_to_csv()
            
            print(f"\nâœ… SCRAPING FORCÃ‰ TERMINÃ‰ !")
            print(f"ğŸ“Š {len(self.bde_data)} BDE traitÃ©s au total")
            
        except Exception as e:
            print(f"âŒ ERREUR CRITIQUE : {str(e)}")
            
        finally:
            if self.driver:
                print("\nğŸ”’ Fermeture du navigateur...")
                self.driver.quit()

def main():
    """
    Fonction principale
    """
    print("=" * 60)
    print("ğŸ“ SCRAPER BDE HELLOASSO - FORÃ‡AGE MULTI-PAGES")
    print("=" * 60)
    
    scraper = BDEScraperForced()
    
    # Lancement du scraping forcÃ© Ã  partir de la page 2
    scraper.run_forced_scraping(start_page=2, max_pages=10)
    
    print("\n" + "=" * 60)
    print("âœ… FIN DU PROGRAMME")
    print("=" * 60)

if __name__ == "__main__":
    main() 