"""
Script d'analyse am√©lior√© pour HelloAsso
Version 2 avec gestion des sessions et contournement des blocages
"""

import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import time
import json
from config import BASE_URL, SEARCH_PARAMS, HEADERS

def create_session():
    """
    Cr√©e une session HTTP avec retry et cookies
    """
    session = requests.Session()
    
    # Configuration des tentatives automatiques en cas d'√©chec
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # Application des headers
    session.headers.update(HEADERS)
    
    return session

def analyser_page_helloasso_v2():
    """
    Analyse am√©lior√©e de la page HelloAsso avec diff√©rentes strat√©gies
    """
    print("üîç Analyse am√©lior√©e de la page HelloAsso...")
    
    # Cr√©ation d'une session pour maintenir les cookies
    session = create_session()
    
    try:
        # Strat√©gie 1 : Visiter d'abord la page d'accueil pour r√©cup√©rer les cookies
        print("üìå √âtape 1 : Visite de la page d'accueil...")
        home_response = session.get("https://www.helloasso.com", timeout=10)
        print(f"   Statut page d'accueil : {home_response.status_code}")
        
        # Attendre un peu comme un vrai utilisateur
        time.sleep(2)
        
        # Strat√©gie 2 : Essayer l'URL de recherche
        url = f"{BASE_URL}?category_tags={SEARCH_PARAMS['category_tags']}"
        print(f"\nüìå √âtape 2 : Acc√®s √† la page de recherche...")
        print(f"URL : {url}")
        
        response = session.get(url, timeout=10)
        print(f"‚úÖ Statut de la r√©ponse : {response.status_code}")
        
        if response.status_code == 200:
            # Analyse du contenu
            soup = BeautifulSoup(response.content, 'html.parser')
            
            print(f"\nüìä INFORMATIONS G√âN√âRALES :")
            print(f"   - Titre : {soup.title.text if soup.title else 'Non trouv√©'}")
            print(f"   - Taille HTML : {len(response.content)} caract√®res")
            
            # Recherche de contenu JavaScript (souvent les sites modernes sont en SPA)
            scripts = soup.find_all('script')
            print(f"   - Nombre de scripts JS : {len(scripts)}")
            
            # Recherche de donn√©es JSON dans les scripts
            print("\nüîç RECHERCHE DE DONN√âES JSON :")
            for i, script in enumerate(scripts[:10]):  # Limite aux 10 premiers
                if script.string and ('association' in script.string.lower() or 'data' in script.string.lower()):
                    script_content = script.string[:500]  # Limite pour affichage
                    print(f"   Script {i+1} contient potentiellement des donn√©es :")
                    print(f"   {script_content}...")
            
            # Recherche d'√©l√©ments contenant des associations
            print("\nüîç RECHERCHE D'√âL√âMENTS D'ASSOCIATIONS :")
            
            # S√©lecteurs plus sp√©cifiques √† HelloAsso
            selectors = [
                "div[class*='SearchResult']",
                "div[class*='AssociationCard']", 
                "div[class*='Organization']",
                "div[class*='Card']",
                "article",
                "[data-testid*='association']",
                "[data-testid*='organization']",
                "div[class*='result']"
            ]
            
            associations_trouvees = []
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"   ‚úÖ {len(elements)} √©l√©ments trouv√©s avec : {selector}")
                    associations_trouvees.extend(elements)
                    
                    # Analyse du premier √©l√©ment
                    if elements:
                        premier = elements[0]
                        print(f"      Classes CSS : {premier.get('class', [])}")
                        texte = premier.get_text()[:200].replace('\n', ' ').strip()
                        print(f"      Texte (extrait) : {texte}...")
            
            # Recherche de liens d'associations
            print("\nüîó RECHERCHE DE LIENS :")
            links = soup.find_all('a', href=True)
            association_links = []
            
            for link in links:
                href = link.get('href', '')
                if any(keyword in href.lower() for keyword in ['association', 'organization', 'bde']):
                    association_links.append({
                        'url': href,
                        'text': link.get_text().strip()[:100]
                    })
            
            print(f"   üìÑ {len(association_links)} liens d'associations trouv√©s")
            for i, link in enumerate(association_links[:5]):  # Affiche les 5 premiers
                print(f"      {i+1}. {link['url']} - {link['text']}")
            
            # Sauvegarde pour analyse manuelle
            with open('backend/data/page_analysis_v2.html', 'w', encoding='utf-8') as f:
                f.write(soup.prettify())
            print(f"\nüíæ HTML sauvegard√© dans : backend/data/page_analysis_v2.html")
            
            # Sauvegarde des informations structur√©es
            analysis_data = {
                'status_code': response.status_code,
                'title': soup.title.text if soup.title else None,
                'content_length': len(response.content),
                'scripts_count': len(scripts),
                'associations_found': len(associations_trouvees),
                'links_found': len(association_links),
                'selectors_working': [sel for sel in selectors if soup.select(sel)]
            }
            
            with open('backend/data/analysis_results.json', 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, indent=2, ensure_ascii=False)
            print(f"üíæ R√©sultats d'analyse sauvegard√©s dans : backend/data/analysis_results.json")
            
        elif response.status_code == 403:
            print("‚ùå Acc√®s refus√© (403) - Le site bloque notre requ√™te")
            print("üí° Suggestions :")
            print("   - Le site d√©tecte probablement le scraping")
            print("   - Utilisation de Selenium recommand√©e")
            print("   - Ou recherche d'une API publique")
            
        else:
            print(f"‚ùå Erreur HTTP {response.status_code}")
            
    except requests.exceptions.Timeout:
        print("‚ùå Timeout - Le site met trop de temps √† r√©pondre")
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur de requ√™te : {str(e)}")
    except Exception as e:
        print(f"‚ùå Erreur inattendue : {str(e)}")
    
    finally:
        session.close()

if __name__ == "__main__":
    print("üöÄ Analyse avanc√©e du site HelloAsso")
    analyser_page_helloasso_v2()
    print("‚úÖ Analyse termin√©e !") 