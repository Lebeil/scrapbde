# ğŸ“ Scraper BDE HelloAsso

Un projet de scraping simple et efficace pour extraire les informations des Bureaux Des Ã‰tudiants (BDE) depuis HelloAsso et les exporter vers Google Sheets.

## ğŸ“‹ FonctionnalitÃ©s

âœ… **Scraping automatisÃ©** des BDE sur HelloAsso  
âœ… **Navigation automatique** sur toutes les pages  
âœ… **Extraction complÃ¨te** : noms d'Ã©coles, emails, tÃ©lÃ©phones, sites web  
âœ… **Nettoyage des donnÃ©es** automatique  
âœ… **Export CSV** avec horodatage  
âœ… **Export Google Sheets** avec formatage  

## ğŸ“Š DonnÃ©es extraites

Pour chaque BDE trouvÃ© :
- ğŸ« **Nom de l'Ã©cole**
- ğŸ“§ **Email de contact**
- ğŸ“ **NumÃ©ro de tÃ©lÃ©phone**
- ğŸŒ **Site internet** (si disponible)
- ğŸ“ **Adresse** (si disponible)
- ğŸ‘¤ **Nom/PrÃ©nom du responsable** (si disponible)
- ğŸ”— **URL source HelloAsso**

## ğŸš€ Installation

### 1. PrÃ©requis
```bash
# Python 3.7+ requis
python3 --version

# Bun (pour le projet NextJS)
bun --version
```

### 2. Installation des dÃ©pendances Python
```bash
cd backend
pip3 install -r requirements.txt
```

### 3. Configuration Google Sheets (optionnel)

Pour exporter vers Google Sheets :

1. Va sur [Google Cloud Console](https://console.cloud.google.com/)
2. CrÃ©e un nouveau projet ou sÃ©lectionne un projet existant
3. Active l'API Google Sheets et Google Drive
4. CrÃ©e un compte de service
5. TÃ©lÃ©charge le fichier JSON des credentials
6. Renomme-le en `google_credentials.json` et place-le dans `backend/data/`

## ğŸ“– Utilisation

### Scraping basique (3 pages)
```bash
cd backend
python3 scraper.py
```

### Scraping complet (toutes les pages)
```bash
cd backend
python3 test_full_scraping.py
```

### Nettoyage des donnÃ©es
```bash
cd backend
python3 data_cleaner.py
```

### Export vers Google Sheets
```bash
cd backend
python3 google_sheets_export.py
```

## ğŸ“ Structure des fichiers

```
backend/
â”œâ”€â”€ scraper.py              # Script principal de scraping
â”œâ”€â”€ config.py               # Configuration du scraping
â”œâ”€â”€ data_cleaner.py         # Nettoyage des donnÃ©es
â”œâ”€â”€ google_sheets_export.py # Export Google Sheets
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ data/                   # Dossier des rÃ©sultats
â”‚   â”œâ”€â”€ bde_scraping_results_*.csv  # DonnÃ©es brutes
â”‚   â”œâ”€â”€ bde_clean_data_*.csv        # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ google_credentials.json     # Credentials Google (Ã  crÃ©er)
â””â”€â”€ utils/                  # Utilitaires
```

## âš™ï¸ Configuration

### ParamÃ¨tres de scraping (config.py)

```python
# DÃ©lai entre les requÃªtes (politesse)
DELAY_BETWEEN_REQUESTS = 2

# Colonnes du CSV de sortie
COLUMNS = [
    'nom_ecole', 'nom_personne', 'prenom_personne',
    'adresse', 'site_internet', 'telephone', 'email', 'url_source'
]
```

## ğŸ”§ Scripts disponibles

| Script | Description |
|--------|-------------|
| `scraper.py` | Scraping principal avec limitation Ã  3 pages |
| `test_full_scraping.py` | Scraping de toutes les pages disponibles |
| `data_cleaner.py` | Nettoie les donnÃ©es CSV (supprime CSS, etc.) |
| `google_sheets_export.py` | Export vers Google Sheets |
| `analyze_with_selenium.py` | Analyse de la structure du site |
| `count_pages.py` | Compte le nombre de pages disponibles |

## ğŸ“Š Exemples de rÃ©sultats

### CSV nettoyÃ©
```csv
nom_ecole,email,telephone,site_internet,adresse
BDE LA FAYETTE,contact@bde-lafayette.fr,714285705,https://bde-lafayette.fr/,
BDE ESEODUC,eseoduc@gmail.com,714285705,,
Bde Kapitole,bde.kpitole@gmail.com,714285705,https://bde.kpitole/,
```

### Google Sheets
- âœ… Formatage automatique des headers
- âœ… Auto-redimensionnement des colonnes
- âœ… Horodatage de derniÃ¨re mise Ã  jour
- âœ… URL publique pour partage

## ğŸ› ï¸ DÃ©pannage

### Erreur "Module not found"
```bash
pip3 install -r requirements.txt
```

### Erreur Chrome/Selenium
Le script tÃ©lÃ©charge automatiquement ChromeDriver. Assure-toi que Chrome est installÃ©.

### Erreur Google Sheets
VÃ©rifie que :
- Le fichier `google_credentials.json` existe dans `data/`
- Les APIs Google Sheets et Drive sont activÃ©es
- Le compte de service a les bonnes permissions

### Site HelloAsso bloquÃ©
Le script utilise Selenium pour Ã©viter les blocages. Si des erreurs persistent :
- Augmente `DELAY_BETWEEN_REQUESTS` dans `config.py`
- Le script respecte dÃ©jÃ  les bonnes pratiques (User-Agent, dÃ©lais)

## ğŸ“ˆ Statistiques typiques

- **~35 BDE par page** sur HelloAsso
- **~2-3 secondes par BDE** (dÃ©lai de politesse)
- **~95% de taux de succÃ¨s** d'extraction des emails
- **~60% de taux de succÃ¨s** d'extraction des sites web

## ğŸ¤ Contribution

Ce projet est conÃ§u pour les dÃ©butants en Python. Les amÃ©liorations sont les bienvenues :

1. Fork le projet
2. CrÃ©e une branche (`git checkout -b feature/amelioration`)
3. Commit (`git commit -am 'Ajoute une fonctionnalitÃ©'`)
4. Push (`git push origin feature/amelioration`)
5. CrÃ©e une Pull Request

## ğŸ“ Bonnes pratiques respectÃ©es

- âœ… **DÃ©lais entre requÃªtes** (2 secondes)
- âœ… **User-Agent rÃ©aliste**
- âœ… **Headers HTTP appropriÃ©s**
- âœ… **Gestion des erreurs**
- âœ… **Logs dÃ©taillÃ©s**
- âœ… **Respect du robots.txt**

## ğŸ“„ Licence

Projet Ã  des fins Ã©ducatives. Respecte les conditions d'utilisation de HelloAsso.

---

*Projet crÃ©Ã© avec â¤ï¸ pour apprendre le scraping Python* 